---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

vars:
  destinationTemplate: "{{.ROOT_DIR}}/.taskfiles/VolSync/ReplicationDestination.tmpl.yaml"
  wipeJobTemplate: "{{.ROOT_DIR}}/.taskfiles/VolSync/WipeJob.tmpl.yaml"
  waitForJobScript: "{{.ROOT_DIR}}/.taskfiles/VolSync/wait-for-job.sh"
  listJobTemplate: "{{.ROOT_DIR}}/.taskfiles/VolSync/ListJob.tmpl.yaml"
  forgetJobTemplate: "{{.ROOT_DIR}}/.taskfiles/VolSync/ForgetJob.tmpl.yaml"
  unlockJobTemplate: "{{.ROOT_DIR}}/.taskfiles/VolSync/UnlockJob.tmpl.yaml"
  ts: '{{now | date "150405"}}'

env:
  rsrc: '{{.rsrc}}'
  controller: '{{.controller}}'
  namespace: '{{.namespace}}'
  claim: '{{.claim}}'
  ts: '{{.ts}}'
  kustomization: '{{.kustomization}}'
  previous: '{{.previous}}'

tasks:

  list:
    desc: List all snapshots taken by restic for a given ReplicationSource (ex. task vs:list rsrc=plex [namespace=default])
    silent: true
    cmds:
      - envsubst < <(cat {{.listJobTemplate}}) | kubectl apply -f -
      - bash {{.waitForJobScript}} list-{{.rsrc}}-{{.ts}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/list-{{.rsrc}}-{{.ts}} --for condition=complete --timeout=1m
      - kubectl -n {{.namespace}} logs job/list-{{.rsrc}}-{{.ts}} --container list
      - kubectl -n {{.namespace}} delete job list-{{.rsrc}}-{{.ts}}
    vars:
      rsrc: '{{ or .rsrc (fail "ReplicationSource `rsrc` is required") }}'
      namespace: '{{.namespace | default "default"}}'
    preconditions:
      - sh: test -f {{.waitForJobScript}}
      - sh: test -f {{.listJobTemplate}}

  forget:
    desc: Forget (delete) old snapshots from restic repository, keeping only last 3 snapshots (ex. task vs:forget rsrc=plex [namespace=default])
    silent: true
    cmds:
      - |
        export rsrc="{{.rsrc}}"
        export ts="{{.ts}}"
        export namespace="{{.namespace}}"
        envsubst < <(cat {{.forgetJobTemplate}}) | kubectl apply -f -
      - bash {{.waitForJobScript}} forget-{{.rsrc}}-{{.ts}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/forget-{{.rsrc}}-{{.ts}} --for condition=complete --timeout=10m
      - kubectl -n {{.namespace}} logs job/forget-{{.rsrc}}-{{.ts}} --container forget
      - kubectl -n {{.namespace}} delete job forget-{{.rsrc}}-{{.ts}}
    vars:
      rsrc: '{{ or .rsrc (fail "ReplicationSource `rsrc` is required") }}'
      namespace: '{{.namespace | default "default"}}'
    preconditions:
      - sh: test -f {{.waitForJobScript}}
      - sh: test -f {{.forgetJobTemplate}}

  unlock:
    desc: Unlocks restic repository for a given ReplicationSource (ex. task vs:unlock rsrc=plex [namespace=default])
    silent: true
    cmds:
      - envsubst < <(cat {{.unlockJobTemplate}}) | kubectl apply -f -
      - bash {{.waitForJobScript}} unlock-{{.rsrc}}-{{.ts}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/unlock-{{.rsrc}}-{{.ts}} --for condition=complete --timeout=1m
      - kubectl -n {{.namespace}} logs job/unlock-{{.rsrc}}-{{.ts}} --container unlock
      - kubectl -n {{.namespace}} delete job unlock-{{.rsrc}}-{{.ts}}
    vars:
      rsrc: '{{ or .rsrc (fail "ReplicationSource `rsrc` is required") }}'
      namespace: '{{.namespace | default "default"}}'
    preconditions:
      - sh: test -f {{.waitForJobScript}}
      - sh: test -f {{.unlockJobTemplate}}

  # To run backup jobs in parallel for all replicationsources:
  #  - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | xargs --max-procs=4 -l bash -c 'task vs:snapshot rsrc=$0 namespace=$1'
  #
  snapshot:
    desc: Trigger a Restic ReplicationSource snapshot (ex. task vs:snapshot rsrc=plex [namespace=default])
    cmds:
      - kubectl -n {{.namespace}} patch replicationsources {{.rsrc}} --type merge -p '{"spec":{"trigger":{"manual":"{{.ts}}"}}}'
      - bash {{.waitForJobScript}} volsync-src-{{.rsrc}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/volsync-src-{{.rsrc}} --for condition=complete --timeout=120m
      # TODO: Error from server (NotFound): jobs.batch "volsync-src-zzztest" not found
      # - kubectl -n {{.namespace}} logs job/volsync-src-{{.rsrc}}
    vars:
      rsrc: '{{ or .rsrc (fail "ReplicationSource `rsrc` is required") }}'
      namespace: '{{.namespace | default "default"}}'
    preconditions:
      - sh: test -f {{.waitForJobScript}}
      - sh: kubectl -n {{.namespace}} get replicationsources {{.rsrc}}
        msg: "ReplicationSource '{{.rsrc}}' not found in namespace '{{.namespace}}'"

  # To run restore jobs in parallel for all replicationdestinations:
  #   - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | xargs --max-procs=2 -l bash -c 'task vs:restore rsrc=$0 namespace=$1'
  #
  restore:
    desc: Restore a PVC from restic backup (ex. task vs:restore rsrc=plex [namespace=default] [previous=N] - where N is the snapshot index to restore, default=1 for latest)
    cmds:
      - task: restore-suspend-app
        vars:
          rsrc: '{{.rsrc}}'
          namespace: '{{.namespace}}'
          kustomization: '{{.kustomization}}'
          controller: '{{.controller}}'
      - task: restore-wipe-job
        vars:
          rsrc: '{{.rsrc}}'
          namespace: '{{.namespace}}'
          claim: '{{.claim}}'
          ts: '{{.ts}}'
      - task: restore-volsync-job
        vars:
          rsrc: '{{.rsrc}}'
          namespace: '{{.namespace}}'
          claim: '{{.claim}}'
          ts: '{{.ts}}'
          previous: '{{.previous}}'
      - task: restore-resume-app
        vars:
          rsrc: '{{.rsrc}}'
          namespace: '{{.namespace}}'
          kustomization: '{{.kustomization}}'
          controller: '{{.controller}}'
    vars:
      rsrc: '{{ or .rsrc (fail "Variable `rsrc` is required") }}'
      namespace: '{{.namespace | default "default"}}'
      # 1) Query to find the Flux Kustomization associated with the ReplicationSource (rsrc)
      # Falls back to common pattern if label is missing
      kustomization:
        sh: |
          kustomization=$(kubectl -n {{.namespace}} get replicationsource {{.rsrc}} \
            -o jsonpath="{.metadata.labels.kustomize\.toolkit\.fluxcd\.io/name}" 2>/dev/null)
          if [ -z "$kustomization" ]; then
            # Try common naming pattern as fallback
            kustomization="cluster-apps-{{.rsrc}}"
            echo "Warning: Using fallback kustomization name: $kustomization" >&2
          fi
          echo "$kustomization"
      # 2) Query to find the Claim associated with the ReplicationSource (rsrc)
      claim:
        sh: |
          claim=$(kubectl -n {{.namespace}} get replicationsource {{.rsrc}} \
            -o jsonpath="{.spec.sourcePVC}" 2>/dev/null)
          if [ -z "$claim" ]; then
            echo "Error: Could not find PVC for ReplicationSource {{.rsrc}}" >&2
            exit 1
          fi
          echo "$claim"
      # 3) Query to find the controller associated with the PersistentVolumeClaim (claim)
      controller:
        sh: |
          # First get the claim
          claim=$(kubectl -n {{.namespace}} get replicationsource {{.rsrc}} \
            -o jsonpath="{.spec.sourcePVC}" 2>/dev/null)
          if [ -z "$claim" ]; then
            echo "Error: Could not find PVC for ReplicationSource {{.rsrc}}" >&2
            exit 1
          fi
          # Then get the app name from the claim
          app=$(kubectl -n {{.namespace}} get persistentvolumeclaim "$claim" \
            -o jsonpath="{.metadata.labels.app\.kubernetes\.io/name}" 2>/dev/null)
          if [ -z "$app" ]; then
            # Try using the claim name as app name
            app="$claim"
            echo "Warning: Using PVC name as app name: $app" >&2
          fi
          # Check for different controller types
          if kubectl -n {{.namespace}} get deployment.apps/$app >/dev/null 2>&1; then
            echo "deployment.apps/$app"
          elif kubectl -n {{.namespace}} get statefulset.apps/$app >/dev/null 2>&1; then
            echo "statefulset.apps/$app"
          elif kubectl -n {{.namespace}} get daemonset.apps/$app >/dev/null 2>&1; then
            echo "daemonset.apps/$app"
          else
            echo "Error: Could not find controller for app: $app" >&2
            exit 1
          fi
      previous: "{{.previous | default 1}}"
    preconditions:
      - sh: test -f {{.wipeJobTemplate}}
        msg: "Wipe job template not found"
      - sh: test -f {{.destinationTemplate}}
        msg: "Destination template not found"
      - sh: test -f {{.waitForJobScript}}
        msg: "Wait for job script not found"
      - sh: kubectl -n {{.namespace}} get replicationsource {{.rsrc}} >/dev/null 2>&1
        msg: "ReplicationSource '{{.rsrc}}' not found in namespace '{{.namespace}}'"
      - sh: |
          claim=$(kubectl -n {{.namespace}} get replicationsource {{.rsrc}} -o jsonpath="{.spec.sourcePVC}" 2>/dev/null)
          kubectl -n {{.namespace}} get pvc "$claim" >/dev/null 2>&1
        msg: "PVC for ReplicationSource '{{.rsrc}}' not found"

  # Suspend the Flux ks and hr
  restore-suspend-app:
    internal: true
    vars:
      rsrc: '{{.rsrc}}'
      namespace: '{{.namespace}}'
      kustomization: '{{.kustomization}}'
      controller: '{{.controller}}'
    cmds:
      - flux -n flux-system suspend kustomization {{.kustomization}}
      - flux -n {{.namespace}} suspend helmrelease {{.rsrc}}
      - kubectl -n {{.namespace}} scale {{.controller}} --replicas 0
      - kubectl -n {{.namespace}} wait pod --for delete --selector="app.kubernetes.io/name={{.rsrc}}" --timeout=2m
      # Double-check that pods are really gone
      - sleep 5
      - |
        pod_count=$(kubectl -n {{.namespace}} get pods -l "app.kubernetes.io/name={{.rsrc}}" --no-headers | wc -l)
        if [ "$pod_count" -gt 0 ]; then
          echo "Warning: Still found $pod_count pod(s) after scale down" >&2
          kubectl -n {{.namespace}} delete pods -l "app.kubernetes.io/name={{.rsrc}}" --force --grace-period=0
          sleep 10
        fi

  # Wipe the PVC of all data
  restore-wipe-job:
    internal: true
    vars:
      rsrc: '{{.rsrc}}'
      namespace: '{{.namespace}}'
      claim: '{{.claim}}'
      ts: '{{.ts}}'
    env:
      rsrc: '{{.rsrc}}'
      namespace: '{{.namespace}}'
      claim: '{{.claim}}'
      ts: '{{.ts}}'
    cmds:
      - envsubst < <(cat {{.wipeJobTemplate}}) | kubectl apply -f -
      - bash {{.waitForJobScript}} wipe-{{.rsrc}}-{{.claim}}-{{.ts}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/wipe-{{.rsrc}}-{{.claim}}-{{.ts}} --for condition=complete --timeout=120m
      - kubectl -n {{.namespace}} logs job/wipe-{{.rsrc}}-{{.claim}}-{{.ts}} --container wipe
      - kubectl -n {{.namespace}} delete job wipe-{{.rsrc}}-{{.claim}}-{{.ts}}

  # Create VolSync replicationdestination CR to restore data
  restore-volsync-job:
    internal: true
    vars:
      rsrc: '{{.rsrc}}'
      namespace: '{{.namespace}}'
      claim: '{{.claim}}'
      ts: '{{.ts}}'
      previous: '{{.previous}}'
    env:
      rsrc: '{{.rsrc}}'
      namespace: '{{.namespace}}'
      claim: '{{.claim}}'
      ts: '{{.ts}}'
      previous: '{{.previous}}'
    cmds:
      - envsubst < <(cat {{.destinationTemplate}}) | kubectl apply -f -
      - bash {{.waitForJobScript}} volsync-dst-{{.rsrc}}-{{.claim}}-{{.ts}} {{.namespace}}
      - kubectl -n {{.namespace}} wait job/volsync-dst-{{.rsrc}}-{{.claim}}-{{.ts}} --for condition=complete --timeout=120m
      - kubectl -n {{.namespace}} delete replicationdestination {{.rsrc}}-{{.claim}}-{{.ts}}

  # Resume Flux ks and hr
  restore-resume-app:
    internal: true
    vars:
      rsrc: '{{.rsrc}}'
      namespace: '{{.namespace}}'
      kustomization: '{{.kustomization}}'
      controller: '{{.controller}}'
    cmds:
      - flux -n {{.namespace}} resume helmrelease {{.rsrc}}
      - flux -n flux-system resume kustomization {{.kustomization}}
      # Explicitly scale the controller back up to 1 replica
      - kubectl -n {{.namespace}} scale {{.controller}} --replicas 1
      # Wait for the controller to be scaled back up
      - |
        echo "Waiting for {{.controller}} to be scaled up..."
        kubectl -n {{.namespace}} wait --for=jsonpath='{.spec.replicas}'=1 {{.controller}} --timeout=2m || true
      # Wait for pod to be ready
      - kubectl -n {{.namespace}} wait pod --for=condition=ready --selector="app.kubernetes.io/name={{.rsrc}}" --timeout=5m
